{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MDbeL0tgPxr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyhlFgkEgXa1"
      },
      "source": [
        "Step 1: Mount Google Drive (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1eFQ0ZLgYOg",
        "outputId": "3d525a85-e68e-4ad8-bed2-91a4d215baa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z1pjqR_giQs"
      },
      "source": [
        "Step 2: Import Necessary Libraries\n",
        "python\n",
        "Copy code\n",
        "*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAQChgCJgabC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4eRC9obgosc"
      },
      "source": [
        "Step 3: Define the Custom Dataset\n",
        "For a DCGAN, you typically don't need paired datasets, so only a raw image dataset is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc4zHgrQgjJp"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_names = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_names[idx])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bFvf8YhP5i"
      },
      "source": [
        "Step 4: Define Data Transformations and Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBbve-ttgqxo"
      },
      "outputs": [],
      "source": [
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # For DCGAN, standard size is 64x64\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for tanh activation\n",
        "])\n",
        "\n",
        "# Directory path\n",
        "image_dir = '/content/drive/MyDrive/DIP_Project/raw-890/'\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = ImageDataset(image_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZERiVP0bhseb"
      },
      "source": [
        "Step 5: Define the Generator and Discriminator for DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbmCSEmAhgkJ"
      },
      "outputs": [],
      "source": [
        "class DCGANGenerator(nn.Module):\n",
        "    def __init__(self, z_dim=100, ngf=64, output_nc=3):\n",
        "        super(DCGANGenerator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, output_nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjLInm78mIoU"
      },
      "outputs": [],
      "source": [
        "class DCGANDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc=3, ndf=64):\n",
        "        super(DCGANDiscriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1lVXGJehzr_"
      },
      "source": [
        "Step 6: Initialize Models, Loss Functions, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxEQ4AMrhwy0",
        "outputId": "086b95c2-1335-4721-ab84-a410794463eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Initialize models\n",
        "netG = DCGANGenerator(z_dim=100).to(device)\n",
        "netD = DCGANDiscriminator().to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QafsGDnKkDou"
      },
      "source": [
        "Step 7: Define Checkpointing Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGqPPqokh1S5"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/DIP_Project/checkpoints_DCGAN/'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(modelG, modelD, optimizer_G, optimizer_D, checkpoint_dir):\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
        "    if not checkpoints:\n",
        "        print(\"No checkpoints found, starting from scratch.\")\n",
        "        return 0\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[1].split('.pth')[0]))\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
        "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    modelG.load_state_dict(checkpoint['modelG_state_dict'])\n",
        "    modelD.load_state_dict(checkpoint['modelD_state_dict'])\n",
        "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_vHNhyomQjS"
      },
      "source": [
        "Step 8: Load Checkpoint if Available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LH0tFH8j6-B",
        "outputId": "08942ec3-be2f-4e0b-99b6-df6d23fbaaeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint: /content/drive/MyDrive/DIP_Project/checkpoints_DCGAN/checkpoint_200.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-dbba47e560a8>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from epoch 200\n"
          ]
        }
      ],
      "source": [
        "start_epoch = load_checkpoint(netG, netD, optimizer_G, optimizer_D, checkpoint_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuwAKxfrmcro"
      },
      "source": [
        "Step 9: Define the Training Loop with Periodic Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVgMixy-pV6M",
        "outputId": "458b32d1-455f-42b6-c843-e33ba0daa0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure that these components are already defined:\n",
        "# - netG: The generator model\n",
        "# - netD: The discriminator model\n",
        "# - dataloader: The DataLoader for your training data\n",
        "# - device: The device (e.g., 'cuda' or 'cpu')\n",
        "# - checkpoint_dir: The directory where checkpoints are stored\n",
        "\n",
        "# Define the GAN loss criterion (Binary Cross-Entropy loss)\n",
        "criterion_GAN = nn.BCELoss()\n",
        "\n",
        "# Define the L1 loss criterion for pixel-level comparison\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "# Define a function to save the model checkpoint\n",
        "def save_checkpoint(state, filename='checkpoint.pth'):\n",
        "    torch.save(state, filename)\n",
        "    print(f\"Checkpoint saved at {filename}\")\n",
        "\n",
        "# Function to visualize generated images\n",
        "def show_generated_images(epoch, generator, num_images=5):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_images, 100, 1, 1).to(device)  # Random latent vectors\n",
        "        generated_images = generator(z)\n",
        "        fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "        for i in range(num_images):\n",
        "            img = generated_images[i].cpu().squeeze().permute(1, 2, 0) * 0.5 + 0.5  # Rescale to [0, 1]\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].axis('off')\n",
        "        plt.suptitle(f\"Generated Images at Epoch {epoch}\")\n",
        "        plt.show()\n",
        "\n",
        "# Total number of epochs and checkpoint saving interval\n",
        "total_epochs = 200\n",
        "save_every = 10  # Save a checkpoint every 10 epochs\n",
        "start_epoch   # Adjust this if resuming from a saved checkpoint\n",
        "\n",
        "# Optimizers for the generator and discriminator\n",
        "optimizer_G = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "\n",
        "    epoch_loss_D = 0.0\n",
        "    epoch_loss_G = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Progress bar for the current epoch\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\", leave=False)\n",
        "\n",
        "    for i, real_images in enumerate(progress_bar):\n",
        "        real_images = real_images.to(device)  # Move images to the device (GPU/CPU)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real images\n",
        "        pred_real = netD(real_images)\n",
        "        target_real = torch.ones_like(pred_real).to(device)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "\n",
        "        # Fake images generated by the generator\n",
        "        batch_size = real_images.size(0)\n",
        "        z = torch.randn(batch_size, 100, 1, 1).to(device)  # Random latent vector\n",
        "        fake = netG(z)\n",
        "        pred_fake = netD(fake.detach())  # Detach to avoid updating the generator\n",
        "        target_fake = torch.zeros_like(pred_fake).to(device)\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        pred_fake = netD(fake)\n",
        "        target_G = torch.ones_like(pred_fake).to(device)\n",
        "        loss_G_GAN = criterion_GAN(pred_fake, target_G)\n",
        "        loss_G_L1 = criterion_L1(fake, real_images) * 100\n",
        "        loss_G = loss_G_GAN + loss_G_L1\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_loss_D += loss_D.item()\n",
        "        epoch_loss_G += loss_G.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'Loss_D': loss_D.item(), 'Loss_G': loss_G.item()})\n",
        "\n",
        "    avg_loss_D = epoch_loss_D / num_batches\n",
        "    avg_loss_G = epoch_loss_G / num_batches\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}] Loss D: {avg_loss_D:.4f}, Loss G: {avg_loss_G:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % save_every == 0 or (epoch + 1) == total_epochs:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{epoch+1}.pth')\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch,\n",
        "            'modelG_state_dict': netG.state_dict(),\n",
        "            'modelD_state_dict': netD.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Display generated images periodically\n",
        "    if (epoch + 1) % save_every == 0 or (epoch + 1) == total_epochs:\n",
        "        show_generated_images(epoch + 1, netG)\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyNnBSWjqmeQ",
        "outputId": "6922d310-f976-4f1c-a6a0-40bc7a727cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final models saved.\n"
          ]
        }
      ],
      "source": [
        "# Save final models\n",
        "final_modelG_path = '/content/drive/MyDrive/DIP_Project/DC_GAN_netG_final.pth'\n",
        "final_modelD_path = '/content/drive/MyDrive/DIP_Project/DC_GAN_netD_final.pth'\n",
        "\n",
        "torch.save(netG.state_dict(), final_modelG_path)\n",
        "torch.save(netD.state_dict(), final_modelD_path)\n",
        "\n",
        "print(\"Final models saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PidwbsYhNDi_",
        "outputId": "24dc93fc-6e96-44b9-9de4-dbc612771803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: piq in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision numpy scikit-image piqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEGhYU2v4oe-",
        "outputId": "ec2a0a1f-ac0f-4361-83ef-9871b0822225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch-fidelity) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: torch-fidelity\n",
            "Successfully installed torch-fidelity-0.3.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-fidelity\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byv0m1dG4pjw",
        "outputId": "f0e3b169-ed63-487a-af68-dc68bb9dabf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics[image] in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (0.11.8)\n",
            "Requirement already satisfied: torch-fidelity<=0.4.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (0.3.0)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (0.20.1+cu121)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]) (1.13.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics[image]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics[image]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics[image]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics[image]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics[image]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics[image]) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics[image]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchmetrics[image]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5vlSMvv4sTi"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torch.nn.functional import mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDDuvXcC4cLg",
        "outputId": "a896c8ea-48c5-42c8-9f6d-db553f628d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-43-cb674309b158>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  netG.load_state_dict(torch.load('/content/drive/MyDrive/DIP_Project/DC_GAN_netG_final.pth', map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
            "  _future_warning(\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 108MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 0.0647\n",
            "PSNR: 11.8931\n",
            "SSIM: 0.1725\n",
            "FID: 238.7355\n",
            "UIQM: 85.5271\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchmetrics import MeanSquaredError, PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom dataset class for loading raw and reference images\n",
        "class UIEB_Dataset(Dataset):\n",
        "    def __init__(self, raw_dir, reference_dir, transform=None):\n",
        "        self.raw_dir = raw_dir\n",
        "        self.reference_dir = reference_dir\n",
        "        self.image_names = sorted(os.listdir(raw_dir))  # Ensure consistent ordering\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw_image_path = os.path.join(self.raw_dir, self.image_names[idx])\n",
        "        reference_image_path = os.path.join(self.reference_dir, self.image_names[idx])\n",
        "\n",
        "        raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "        reference_image = Image.open(reference_image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            raw_image = self.transform(raw_image)\n",
        "            reference_image = self.transform(reference_image)\n",
        "\n",
        "        return raw_image, reference_image\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Adjust size as needed\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Directory paths\n",
        "raw_dir = '/content/drive/MyDrive/DIP_Project/raw-890/'\n",
        "reference_dir = '/content/drive/MyDrive/DIP_Project/reference-890/'\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = UIEB_Dataset(raw_dir, reference_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the model (ensure this matches your DCGAN generator structure)\n",
        "netG = DCGANGenerator(z_dim=100).to(device)\n",
        "netG.load_state_dict(torch.load('/content/drive/MyDrive/DIP_Project/DC_GAN_netG_final.pth', map_location=device))\n",
        "netG.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize metrics\n",
        "mse = MeanSquaredError().to(device)\n",
        "psnr = PeakSignalNoiseRatio().to(device)\n",
        "ssim = StructuralSimilarityIndexMeasure().to(device)\n",
        "fid = FrechetInceptionDistance().to(device)\n",
        "\n",
        "# Function to convert image tensor for FID metric\n",
        "def convert_to_uint8_for_fid(image_tensor):\n",
        "    image_tensor = (image_tensor * 255).clamp(0, 255)  # Rescale from [0, 1] to [0, 255]\n",
        "    return image_tensor.to(torch.uint8)\n",
        "\n",
        "# Function to calculate UICM (Colorfulness)\n",
        "def calculate_uicm(image):\n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "    L, A, B = cv2.split(lab_image)\n",
        "    colorfulness = np.sqrt(np.mean(A**2) + np.mean(B**2))\n",
        "    return colorfulness\n",
        "\n",
        "# Function to calculate UISM (Sharpness)\n",
        "def calculate_uism(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    sharpness = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
        "    return sharpness\n",
        "\n",
        "# Function to calculate UIConM (Contrast)\n",
        "def calculate_uiconm(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    contrast = gray_image.std()\n",
        "    return contrast\n",
        "\n",
        "# Function to calculate UIQM by combining UICM, UISM, and UIConM\n",
        "def calculate_uiqm(image):\n",
        "    uicm = calculate_uicm(image)\n",
        "    uism = calculate_uism(image)\n",
        "    uiconm = calculate_uiconm(image)\n",
        "    uiqm = 0.5 * uicm + 0.3 * uism + 0.2 * uiconm\n",
        "    return uiqm\n",
        "\n",
        "# Function to process the image tensor and convert it to a numpy array\n",
        "def tensor_to_numpy(image_tensor):\n",
        "    image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
        "    return image\n",
        "\n",
        "# Iterate through the dataloader\n",
        "uiqm_scores = []\n",
        "\n",
        "for batch in dataloader:\n",
        "    raw_images, reference_images = batch\n",
        "    raw_images, reference_images = raw_images.to(device), reference_images.to(device)\n",
        "\n",
        "    # Generate images using the model\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(raw_images.size(0), 100, 1, 1).to(device)  # Latent vector for DCGAN\n",
        "        generated_images = netG(z)\n",
        "\n",
        "    # Scale from [-1, 1] to [0, 1] for MSE, PSNR, SSIM\n",
        "    generated_images_scaled = (generated_images * 0.5 + 0.5)\n",
        "\n",
        "    # Update metrics (MSE, PSNR, SSIM)\n",
        "    mse.update(generated_images_scaled, reference_images)\n",
        "    psnr.update(generated_images_scaled, reference_images)\n",
        "    ssim.update(generated_images_scaled, reference_images)\n",
        "\n",
        "    # Convert images for FID metric\n",
        "    generated_images_uint8 = convert_to_uint8_for_fid(generated_images_scaled)\n",
        "    reference_images_uint8 = convert_to_uint8_for_fid(reference_images)\n",
        "\n",
        "    # Update FID metric\n",
        "    fid.update(generated_images_uint8, real=False)\n",
        "    fid.update(reference_images_uint8, real=True)\n",
        "\n",
        "    # Calculate UIQM for each image in the batch\n",
        "    for img in generated_images_scaled:\n",
        "        img_numpy = tensor_to_numpy(img)\n",
        "        uiqm_value = calculate_uiqm(img_numpy)\n",
        "        uiqm_scores.append(uiqm_value)\n",
        "\n",
        "# Compute final scores\n",
        "mse_score = mse.compute().item()\n",
        "psnr_score = psnr.compute().item()\n",
        "ssim_score = ssim.compute().item()\n",
        "fid_score = fid.compute().item()\n",
        "uiqm_score = np.mean(uiqm_scores)\n",
        "\n",
        "# Print results\n",
        "print(f\"MSE: {mse_score:.4f}\")\n",
        "print(f\"PSNR: {psnr_score:.4f}\")\n",
        "print(f\"SSIM: {ssim_score:.4f}\")\n",
        "print(f\"FID: {fid_score:.4f}\")\n",
        "print(f\"UIQM: {uiqm_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6Vr80qx45yl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}